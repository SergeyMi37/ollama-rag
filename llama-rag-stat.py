import os, sys
import time
from datetime import timedelta
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings
from llama_index.embeddings.ollama import OllamaEmbedding
from llama_index.llms.ollama import Ollama
from llama_index.core.response_synthesizers import ResponseMode

class RAGSystem:
    def __init__(
        self,
        docs_directory: str = "docs/",
        embedding_model: str = "nomic-embed-text",
        llm_model: str = "llama2",
        ollama_url: str = None,
        chunk_size: int = 1000,
        chunk_overlap: int = 100
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAG —Å–∏—Å—Ç–µ–º—ã —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é –º–Ω–æ–≥–æ–∫—Ä–∞—Ç–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤.
        
        Args:
            docs_directory (str): –ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
            embedding_model (str): –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
            llm_model (str): –ù–∞–∑–≤–∞–Ω–∏–µ LLM-–º–æ–¥–µ–ª–∏
            ollama_url (str): URL Ollama —Å–µ—Ä–≤–µ—Ä–∞
            chunk_size (int): –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            chunk_overlap (int): –ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏
        """
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Ollama URL
        if ollama_url is None:
            ollama_url = os.environ.get('OLLAMA_URL', "http://127.0.0.1:11434")
        
        self.ollama_url = ollama_url
        self.docs_directory = docs_directory
        self.embedding_model_name = embedding_model
        self.llm_model_name = llm_model
        self.query_stats = []  # –î–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∑–∞–ø—Ä–æ—Å–æ–≤
        
        print(f'Ollama URL: {ollama_url}')
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π
        self._initialize_models(embedding_model, llm_model, chunk_size, chunk_overlap)
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞
        self._load_documents_and_build_index()
    
    def _initialize_models(self, embedding_model: str, llm_model: str, chunk_size: int, chunk_overlap: int):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è embedding –º–æ–¥–µ–ª–∏ –∏ LLM"""
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è embedding-–º–æ–¥–µ–ª–∏
        self.embed_model = OllamaEmbedding(
            model_name=embedding_model,
            base_url=self.ollama_url,
            timeout=120.0
        )
        print(f'Embedding model: {embedding_model} initialized')
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM
        self.llm = Ollama(
            model=llm_model,
            base_url=self.ollama_url,
            request_timeout=120.0
        )
        print(f'LLM model: {llm_model} initialized')
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≥–ª–æ–±–∞–ª—å–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        Settings.llm = self.llm
        Settings.embed_model = self.embed_model
        Settings.chunk_size = chunk_size
        Settings.chunk_overlap = chunk_overlap
        # Settings.—Åhunk_length_to_split_on = lambda doc: ()
        # Settings.length_function=len
        Settings.separators=["\n\n", "\n", ". ", "! ", "? ", " ", ""] # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–π
        print('Settings configured')
    
    def _load_documents_and_build_index(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞"""
        start_time = time.time()
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        self.documents = SimpleDirectoryReader(self.docs_directory).load_data()
        load_time = time.time() - start_time
        
        print(f'Loaded {len(self.documents)} documents from {self.docs_directory} in {load_time:.2f} seconds')
        
        # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞
        index_start_time = time.time()
        self.index = VectorStoreIndex.from_documents(
            self.documents,
            llm=self.llm,
            embed_model=self.embed_model
        )
        index_time = time.time() - index_start_time
        
        print(f'Vector index built in {index_time:.2f} seconds')
    
    def query(
        self,
        prompt_text: str,
        temperature: float = 0,
        max_tokens: int = 2000,
        similarity_top_k: int = 3,
        timeout: float = 600.0,
        response_mode: ResponseMode = ResponseMode.COMPACT,
        verbose: bool = False
    ) -> dict:
        """
        –í—ã–ø–æ–ª–Ω—è–µ—Ç –∑–∞–ø—Ä–æ—Å –∫ RAG —Å–∏—Å—Ç–µ–º–µ —Å –∏–∑–º–µ—Ä–µ–Ω–∏–µ–º —Å–∫–æ—Ä–æ—Å—Ç–∏.
        
        Args:
            prompt_text (str): –¢–µ–∫—Å—Ç –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
            temperature (float): –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            max_tokens (int): –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –æ—Ç–≤–µ—Ç–µ
            similarity_top_k (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ö–æ–∂–∏—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞
            timeout (float): –¢–∞–π–º–∞—É—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
            response_mode (ResponseMode): –†–µ–∂–∏–º —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è –æ—Ç–≤–µ—Ç–∞
            verbose (bool): –í—ã–≤–æ–¥–∏—Ç—å –ª–∏ –ø–æ–¥—Ä–æ–±–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–∫–æ—Ä–æ—Å—Ç–∏
        
        Returns:
            dict: –°–ª–æ–≤–∞—Ä—å —Å –æ—Ç–≤–µ—Ç–æ–º –∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        """
        if self.index is None:
            raise ValueError("Index not initialized. Call _load_documents_and_build_index() first.")
        
        start_time = time.time()
        
        # –°–æ–∑–¥–∞–Ω–∏–µ query engine —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –¥–ª—è —ç—Ç–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
        query_engine = self.index.as_query_engine(
            response_mode=response_mode,
            timeout=timeout,
            similarity_top_k=similarity_top_k,
            streaming=False,
            llm_kwargs={
                "temperature": temperature,
                "max_tokens": max_tokens
            }
        )
        
        engine_creation_time = time.time() - start_time
        
        # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞
        query_start_time = time.time()
        response = query_engine.query(prompt_text)
        query_execution_time = time.time() - query_start_time
        
        total_time = time.time() - start_time
        
        # –°–±–æ—Ä —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        query_info = {
            "prompt": prompt_text,
            "response": response.response,
            "metrics": {
                "total_time": total_time,
                "engine_creation_time": engine_creation_time,
                "query_execution_time": query_execution_time,
                "similarity_top_k": similarity_top_k,
                "temperature": temperature,
                "max_tokens": max_tokens,
                "timestamp": time.time()
            }
        }
        
        self.query_stats.append(query_info)
        
        if verbose:
            self._print_query_stats(query_info)
        
        return query_info
    
    def _print_query_stats(self, query_info: dict):
        """–í—ã–≤–æ–¥–∏—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–∞"""
        metrics = query_info["metrics"]
        print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–∞:")
        print(f"   üîç –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤: {metrics['similarity_top_k']}")
        print(f"   ‚öôÔ∏è  –°–æ–∑–¥–∞–Ω–∏–µ –¥–≤–∏–∂–∫–∞: {metrics['engine_creation_time']:.2f} —Å–µ–∫")
        print(f"   ü§ñ –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞: {metrics['query_execution_time']:.2f} —Å–µ–∫")
        print(f"   ‚è±Ô∏è  –û–±—â–µ–µ –≤—Ä–µ–º—è: {metrics['total_time']:.2f} —Å–µ–∫")
        print(f"   üìù –î–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞: {len(query_info['response'])} —Å–∏–º–≤–æ–ª–æ–≤")
        print("-" * 60)
    
    def batch_query(self, prompts: list, verbose: bool = True, **kwargs) -> list:
        """
        –í—ã–ø–æ–ª–Ω—è–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –∑–∞–ø—Ä–æ—Å–æ–≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ —Å –∏–∑–º–µ—Ä–µ–Ω–∏–µ–º —Å–∫–æ—Ä–æ—Å—Ç–∏.
        
        Args:
            prompts (list): –°–ø–∏—Å–æ–∫ –ø—Ä–æ–º–ø—Ç–æ–≤
            verbose (bool): –í—ã–≤–æ–¥–∏—Ç—å –ª–∏ –ø–æ–¥—Ä–æ–±–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
            **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è query –º–µ—Ç–æ–¥–∞
        
        Returns:
            list: –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏
        """
        batch_start_time = time.time()
        results = []
        
        for i, prompt in enumerate(prompts):
            if verbose:
                print(f'\nüîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ {i+1}/{len(prompts)}')
            
            result = self.query(prompt, verbose=verbose, **kwargs)
            results.append(result)
        
        batch_time = time.time() - batch_start_time
        if verbose:
            print(f"\nüéØ –ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞:")
            print(f"   üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {len(prompts)}")
            print(f"   ‚è±Ô∏è  –û–±—â–µ–µ –≤—Ä–µ–º—è –ø–∞–∫–µ—Ç–∞: {batch_time:.2f} —Å–µ–∫")
            print(f"   üìà –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –Ω–∞ –∑–∞–ø—Ä–æ—Å: {batch_time/len(prompts):.2f} —Å–µ–∫")
        
        return results

    def get_system_info(self) -> dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–∏—Å—Ç–µ–º–µ"""
        return {
            "ollama_url": self.ollama_url,
            "docs_directory": self.docs_directory,
            "embedding_model": self.embedding_model_name,
            "llm_model": self.llm_model_name,
            "documents_loaded": len(self.documents) if hasattr(self, 'documents') else 0,
            "index_initialized": self.index is not None,
            "total_queries": len(self.query_stats)
        }
    
    def get_performance_stats(self) -> dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–±—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        if not self.query_stats:
            return {"message": "No queries executed yet"}
        
        total_queries = len(self.query_stats)
        total_time = sum(stat["metrics"]["total_time"] for stat in self.query_stats)
        avg_time = total_time / total_queries
        
        return {
            "total_queries": total_queries,
            "total_execution_time": total_time,
            "average_query_time": avg_time,
            "fastest_query": min(stat["metrics"]["total_time"] for stat in self.query_stats),
            "slowest_query": max(stat["metrics"]["total_time"] for stat in self.query_stats),
            "average_response_length": sum(len(stat["response"]) for stat in self.query_stats) / total_queries
        }
    
    def print_detailed_stats(self):
        """–í—ã–≤–æ–¥–∏—Ç –ø–æ–¥—Ä–æ–±–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤—Å–µ—Ö –∑–∞–ø—Ä–æ—Å–æ–≤"""
        if not self.query_stats:
            print("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: –∑–∞–ø—Ä–æ—Å–æ–≤ –µ—â–µ –Ω–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ")
            return
        
        print("\n" + "="*80)
        print("üìà –î–ï–¢–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –í–´–ü–û–õ–ù–ï–ù–ù–´–• –ó–ê–ü–†–û–°–û–í")
        print("="*80)
        
        for i, stat in enumerate(self.query_stats, 1):
            metrics = stat["metrics"]
            print(f"\n#{i} | –í—Ä–µ–º—è: {metrics['total_time']:.2f} —Å–µ–∫ | –¢–æ–∫–µ–Ω—ã: {metrics['max_tokens']} | Temp: {metrics['temperature']}")
            print(f"   –ü—Ä–æ–º–ø—Ç: {stat['prompt'][:100]}...")
            print(f"   –û—Ç–≤–µ—Ç: {stat['response'][:150]}...")
        
        perf_stats = self.get_performance_stats()
        print(f"\n{'='*80}")
        print("üìä –°–í–û–î–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
        print(f"   –í—Å–µ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {perf_stats['total_queries']}")
        print(f"   –û–±—â–µ–µ –≤—Ä–µ–º—è: {perf_stats['total_execution_time']:.2f} —Å–µ–∫")
        print(f"   –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è: {perf_stats['average_query_time']:.2f} —Å–µ–∫")
        print(f"   –°–∞–º—ã–π –±—ã—Å—Ç—Ä—ã–π: {perf_stats['fastest_query']:.2f} —Å–µ–∫")
        print(f"   –°–∞–º—ã–π –º–µ–¥–ª–µ–Ω–Ω—ã–π: {perf_stats['slowest_query']:.2f} —Å–µ–∫")
        print(f"   –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞: {perf_stats['average_response_length']:.0f} —Å–∏–º–≤–æ–ª–æ–≤")

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è —Å–∏—Å—Ç–µ–º—ã (–æ–±—Ä–∞—Ç–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å)
def create_rag_system(
    docs_directory: str = "docs/",
    embedding_model: str = "nomic-embed-text",
    llm_model: str = "llama2",
    ollama_url: str = None
) -> RAGSystem:
    """
    –°–æ–∑–¥–∞–µ—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≥–æ—Ç–æ–≤—É—é RAG —Å–∏—Å—Ç–µ–º—É.
    """
    return RAGSystem(
        docs_directory=docs_directory,
        embedding_model=embedding_model,
        llm_model=llm_model,
        ollama_url=ollama_url
    )

# –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
if __name__ == "__main__":
    # –ï—Å–ª–∏ –≤—ã–∑–≤–∞–Ω–æ –∏–∑ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏ —Å –∞—Ä–≥—É–º–µ–Ω—Ç–∞–º–∏
    if len(sys.argv) > 2:
        embedding_model = sys.argv[1]
        llm_model = sys.argv[2]
        prompt_text = sys.argv[3]
    else:
        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
        embedding_model="nomic-embed-text"
        llm_model="llama2"
        prompt_text = "–ò—Å–ø–æ–ª—å–∑—É—è –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã, –≤—ã–±–µ—Ä–∏ –Ω–æ–º–µ—Ä —Ç–µ–º—ã, –∫–æ—Ç–æ—Ä—ã–π –±–æ–ª—å—à–µ –≤—Å–µ–≥–æ –ø–æ–¥—Ö–æ–¥–∏—Ç –ø–æ —á–∞—Å—Ç–æ—Ç–Ω–æ–º—É –∞–Ω–∞–ª–∏–∑—É —Å–ª–æ–≤ –¥–ª—è —Ç–µ–∫—Å—Ç–∞: '–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤–æ–¥–æ—Å–Ω–∞–±–∂–µ–Ω–∏–µ'. –í –û—Ç–≤–µ—Ç –ø—Ä–µ–¥–æ—Å—Ç–∞–≤—å —Ç–æ–ª—å–∫–æ –Ω–æ–º–µ—Ä —Ç–µ–º—ã."

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã –æ–¥–∏–Ω —Ä–∞–∑
    print("=== –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAG —Å–∏—Å—Ç–µ–º—ã ===")
    rag_system = RAGSystem(
        embedding_model=embedding_model, 
        llm_model=llm_model
    )

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Å–∏—Å—Ç–µ–º–µ
    info = rag_system.get_system_info()
    print("–°–∏—Å—Ç–µ–º–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:", info)
    
    # print("\n=== –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ ===")
    
    # –ü–µ—Ä–≤—ã–π –∑–∞–ø—Ä–æ—Å
    result1 = rag_system.query(
        prompt_text=prompt_text,
        temperature=0,
        max_tokens=1000
    )
    # –í—ã–≤–æ–¥ –æ–±—â–µ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    # rag_system.print_detailed_stats()

    print("–û—Ç–≤–µ—Ç 1:", result1["response"])
    print('–í—ã–ø–æ–ª–Ω—è–ª–æ—Å—å: ',result1["metrics"]["query_execution_time"])
    
    # –í—Ç–æ—Ä–æ–π –∑–∞–ø—Ä–æ—Å
    # result2 = rag_system.query(
    #     prompt_text="–ö—Ä–∞—Ç–∫–æ summarise –æ—Å–Ω–æ–≤–Ω—ã–µ —Ç–µ–º—ã –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.",
    #     temperature=0.1,
    #     max_tokens=500
    # )
    # print("–û—Ç–≤–µ—Ç 2:", result2["response"])
    
    # # –¢—Ä–µ—Ç–∏–π –∑–∞–ø—Ä–æ—Å —Å –¥—Ä—É–≥–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
    # result3 = rag_system.query(
    #     prompt_text="–ù–∞–π–¥–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è—Ö –∏–ª–∏ –º–µ—Ç–æ–¥–∞—Ö.",
    #     similarity_top_k=5,
    #     temperature=0.2
    # )
    # print("–û—Ç–≤–µ—Ç 3:", result3["response"])
    
    # # –ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–æ–≤
    # print("\n=== –ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ ===")
    # prompts = [
    #     "–ö–∞–∫–∏–µ –æ—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –æ–±—Å—É–∂–¥–∞—é—Ç—Å—è –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö?",
    #     "–ö–∞–∫–∏–µ —Ä–µ—à–µ–Ω–∏—è –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç—Å—è?",
    #     "–ö—Ç–æ —è–≤–ª—è–µ—Ç—Å—è —Ü–µ–ª–µ–≤–æ–π –∞—É–¥–∏—Ç–æ—Ä–∏–µ–π –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤?"
    # ]
    
    # batch_results = rag_system.batch_query(
    #     prompts,
    #     temperature=0.1,
    #     max_tokens=800
    # )
    
